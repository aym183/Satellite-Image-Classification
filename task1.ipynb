{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Markdown for Task 1\n",
    "\n",
    "We present a novel dataset based on Sentinel-2 satellite images covering <span style=\"color:blue; font-weight:bold;\">13 spectral bands</span> and consisting out of <span style=\"color:blue; font-weight:bold;\">10 classes with 2 to 3k per class</span> in total <span style=\"color:blue; font-weight:bold;\">27,000 labeled and geo-referenced images</span>. We provide benchmarks for this novel dataset with its spectral bands using state-of-the-art deep Convolutional Neural Network (CNNs). With the proposed novel dataset, we achieved an overall classification accuracy of <span style=\"color:red; font-weight:bold;\">98.57%</span>. The resulting classification system opens a gate towards a number of Earth observation applications. We demonstrate how this classification system can be used for detecting land use and land cover changes and how it can assist in improving geographical maps. The geo-referenced dataset EuroSAT is made publicly available here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to understand:\n",
    "- Spectral bands - different ranges of wavelengths of light that are captured by a satellite sensor\n",
    "- 10 classes\n",
    "- Labeled and geo-referenced images\n",
    "- CNNs\n",
    "- Why is land use and cover classification relevant?\n",
    "- supervised machine learning\n",
    "-  mutli-spectral image data provided by the Sentinel-2A satellite "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Bands and Classes\n",
    "<img title=\"a title\" alt=\"Alt text\" src=\"Images/spectral_bands.png\" width=\"250\" height=\"200\"> <img title=\"a title\" alt=\"Alt text\" src=\"Images/sample_image_patches.png\" width=\"350\" height=\"200\">\n",
    "\n",
    "The three bands B01, B09 and B10 are intended to be used for the correction of atmospheric effects(e.g., aerosols, cirrus or water vapor). The remaining bands are primarily intended to identify and monitor land use and land cover classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Accuracy\n",
    "\n",
    "<img title=\"a title\" alt=\"Alt text\" src=\"Images/classification_accuracy_train_test.png\" width=\"600\" height=\"250\"> <img title=\"a title\" alt=\"Alt text\" src=\"Images/classification_accuracy_best.png\" width=\"350\" height=\"200\"> <img title=\"a title\" alt=\"Alt text\" src=\"Images/classification_accuracy_resnet.png\" width=\"200\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes\n",
    "- The aim of land use and land cover classification is to automatically provide labels describing the represented physical land type or how a land area is used\n",
    "- Data only covers EU countries\n",
    "- In order to improve the chance of getting valuable image patches, we selected satellite images with a low cloud level. Besides the possibility to generate a cloud mask, ESA provides a cloud level value for each satellite image allowing to quickly select images with a low percentage of clouds covering the land scene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips for High Marks\n",
    "- Explore different models in sklearn beyond those covered in class\n",
    "  - Describe conclusions\n",
    "  - Use different metrics and see what you observe (Dont just add all, understand why and show it)\n",
    "- Visualisation examples\n",
    "    - Understand how well your model and feature selection works\n",
    "    - Start with a qn -> hypotheses -> Then BAM\n",
    "- Use functions as much as possible, minimal classes\n",
    "- Print images, accuracies, etc as much as possible (So grader can glance)\n",
    "- NO PANDAS\n",
    "- Run and save notebook using Pickl (?) so on appear, they can see everything\n",
    "\n",
    "## Dont's for Low Marks\n",
    "- Perform pre-processing steps we haven't covered like noise/outlier removal\n",
    "  - Wont get you additional marks\n",
    "- Hyperparameter optimisation -> Explain I am trying to optimise this parameter, this is what it does, and these are the values ive tried\n",
    "  - You can do this manually by looking at a range of values\n",
    "  - Describe the quantities you are optimising\n",
    "  - There are systematic methods like GridSearchCV, DONT USE! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16200, 512)\n",
      "(5400, 512)\n",
      "(16200,)\n",
      "(5400,)\n",
      "[[0.46573916 0.         0.         ... 0.15444896 0.02761412 0.        ]\n",
      " [0.         0.         0.         ... 0.42438483 0.08174433 0.0502976 ]\n",
      " [0.15436319 0.01907348 0.02712335 ... 0.         0.08633541 0.        ]\n",
      " ...\n",
      " [0.         0.22086209 0.07728773 ... 0.11481228 0.1169597  0.        ]\n",
      " [0.         0.         0.15300888 ... 0.         0.04044536 0.00111793]\n",
      " [0.0839657  0.         0.         ... 0.00469289 0.13740638 0.        ]]\n",
      "[0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x_train = np.load(\"Datasets/x_train.npy\")\n",
    "x_test = np.load(\"Datasets/x_test.npy\")\n",
    "y_train = np.load(\"Datasets/y_train.npy\")\n",
    "y_test = np.load(\"Datasets/y_test.npy\")\n",
    "\n",
    "# ((16200 rows, 512 cols) for x_train, (5400 rows, 512 cols) for x_test) | (16200,) -> Represents one dimensional array for y_train, (5400,) -> Represents one dimensional array for for y_test\n",
    "# Train test split -> 75/25\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# All values float\n",
    "\n",
    "print(x_train) # 512 features \n",
    "print(np.unique(y_train)) # [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.] -> 10 classes of possible outputs, and the reoccurences are just one of these values indicating a class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "Write a Python program to pre-process and transform the dataset into a format suitable for developing machine learning models. Use pearson correlation method to identify ten most important features. Use these features to build a SVM classifier and evaluate your modelâ€™s performance.\n",
    "\n",
    "Markdown Question: The difficulty in using pearson correlation method for feature selection is determining the appropriate number of features to select. The method described in the module to automatically choose the appropriate number of features will be computationally very expensive when the number of features is high. Describe an alternative method to reduce the computational complexity of the method discussed in class for datasets with large number of features. Describe the method. No need to write the program.\n",
    "\n",
    "<span style=\"font-weight:bold;\">Save your response (program and markdown) as task1.ipynb.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
